{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "preprocess",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qajx0lAWa1Jb"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "callcount = 0\n",
        "\n",
        "\n",
        "def to_noun_sentence(text):\n",
        "    global callcount\n",
        "    global twitter\n",
        "\n",
        "    callcount += 1\n",
        "    if callcount % 100 == 0:\n",
        "        print(callcount)\n",
        "    stopwords = ['질문', '문의', '관련', '그대로', '계속', '답변', '선생님', '관련문의',\n",
        "            '한지', '자주', '좀', '쪽', '자꾸', '요즘', '몇개', '무조건', '하나요',\n",
        "            '안해','요', '경우', '최근', '및', '몇', '달', '일반', '전날', '저번',\n",
        "            '말', '일어나지', '며칠', '먹기', '지난번', '글', '때문', '너', '무',\n",
        "            '오늘', '시', '잔', '뒤', '지속', '막', '것', '이건', '뭔가', '다시', '그',\n",
        "                '무슨', '안', '난', '도', '기', '후', '거리', '이', '뭘', '저', '뭐', '답젼',\n",
        "                '평생', '회복', '반', '감사', '의사', '보험', '학생', '제발', '살짝',\n",
        "                '느낌', '제', '대해','갑자기','문제', '전','정도', '왜', '거', '가요',\n",
        "                '의심', '어제', '추천', '를', '지금', '무엇', '내일', '관해', '리', '세',\n",
        "                 '로', '목적', '그냥', '거의', '고민', '다음', '이틀', '항상', '뭐', '때',\n",
        "                '요', '가끔', '이후', '혹시', ]\n",
        "    twitter = Okt()\n",
        "    nouns = twitter.nouns(text)\n",
        "    for word in nouns:\n",
        "        if word in stopwords:\n",
        "            while word in nouns:\n",
        "                nouns.remove(word)\n",
        "    return ' '.join(nouns)\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    twitter = Okt()\n",
        "    nouns = twitter.nouns(sentence)\n",
        "    for word in nouns:\n",
        "        if word in stopwords:\n",
        "            while word in nouns:\n",
        "                nouns.remove(word)\n",
        "    nouns.\n",
        "\n",
        "def csv_to_datasets(filepath, vocab_num, sequencelength=10):\n",
        "    import numpy as np \n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import tensorflow.keras as keras\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.utils import class_weight\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "    df['nounlist'] = df['nouns'].str.split()\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    x_train_, x_test_, y_train, y_test = train_test_split(\n",
        "    df['nounlist'], df['label'], \n",
        "    test_size=0.2, random_state=1234, \n",
        "    stratify=df['label']\n",
        "    )\n",
        "\n",
        "    vocab_size = vocab_num\n",
        "    t = Tokenizer(num_words=vocab_size)\n",
        "    t.fit_on_texts(df['nounlist'])\n",
        "\n",
        "    sequence_length = sequencelength\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "\n",
        "    x_train = t.texts_to_sequences(x_train_)\n",
        "    padded_x_train = pad_sequences(x_train, truncating=trunc_type, padding=padding_type, maxlen=sequence_length)\n",
        "    \n",
        "    x_test = t.texts_to_sequences(x_test_)\n",
        "    padded_x_test = pad_sequences(x_test, truncating=trunc_type, padding=padding_type, maxlen=sequence_length)\n",
        "\n",
        "    weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "    weight = {i : weight[i] for i in range(26)}\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train)\n",
        "    y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_csv(filepath, exportpath, startindex=0, endindex=None):\n",
        "    df = pd.read_csv(filepath)\n",
        "    if endindex != None:\n",
        "        df = df[startindex:endindex]\n",
        "    \n",
        "    df.dropna(inplace=True)\n",
        "    df['symptom'] = df['symptom'].str.replace(pat='([ㄱ-ㅎㅏ-ㅣ]+)', repl=r' ', regex=True)\n",
        "    df['symptom'] = df['symptom'].str.replace(pat='[^\\w\\s]', repl=r' ', regex=True)\n",
        "    \n",
        "    df['nouns'] = df['symptom'].apply(to_noun_sentence)\n",
        "    \n",
        "    #라벨 정수화\n",
        "    class_to_label = {'DERM': 0, 'GS': 1, 'IP': 2, 'GI':3, 'OPH':4,\n",
        "                  'NR': 5, 'ENT': 6, 'PSY': 7, 'HEON': 8, 'RHEU': 9,\n",
        "                  'REHM': 10, 'NS': 11, 'AN': 12, 'DENT': 13, 'PS': 14,\n",
        "                  'CS': 15, 'INFC': 16, 'OS': 17,\n",
        "                  'EMR': 18, 'ENDO': 19, 'CA': 20, 'KTM': 21, 'OBGY': 22,\n",
        "                  'URO': 23, 'ALL': 24, 'NPH': 25,'LAB': 26}\n",
        "    df['label'] = df['class'].map(class_to_label)\n",
        "    \n",
        "    # 2개 컬럼만 남김\n",
        "    df = df[['nouns', 'label']]\n",
        "    \n",
        "    # NaN 삭제\n",
        "    df.dropna(inplace=True)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    \n",
        "    df.to_csv(exportpath, encoding='utf-8', index=False)\n",
        "    print('done')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}